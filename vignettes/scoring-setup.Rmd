---
title: "Variant scoring setup and exploration"
author: "Nick Reich"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
set.seed(1717177)
source("../R/helper-functions.R")
```

## Introduction

The goal of this document is to provide an overall conceptual inttroduction to some of the challenges surrounding scoring variant nowcasts.

The following text is from draft guidelines for the variant nowcast hub:

> We will collect nowcasts for $\theta$, a $K$-vector, where $K$ is the number of clades we are interested in, and whose $k$th element, $\theta_k$ , is the true proportion of all current SARS-CoV-2 infections which are clade $k$. We observe $C = (C_1, … , C_K)$, the vector of observed counts for each of the $K$ clades of interest for a particular location and target date, and let $N = \sum_k C_k$ be the total number of sequences collected for that date and location (for simplicity here, we are omitting subscripts for date and location). Variation in $C$ depends on the total number of sequenced samples, $N$. Thus, accurate nowcasts of the observed $C$, would require teams to model and forecast $N$, which is not of epidemiological interest.

> To avoid a situation where the distribution of the prediction target depends on $N$, the total number of sequenced samples on a given day, nowcasts are to be submitted in the form of 100 samples $\hat \theta^{(1)}, …, \hat \theta^{(100)}$ from the predictive distribution for $\theta$. Historical data show that 90 days is sufficient time for nearly all sequences to be tested and reported and therefore for $C$ to represent a stable estimate of relative clade prevalences. Therefore, 90 days after each submission date, the hub will use the total number of observed samples, $N$, and the clade proportion nowcasts $\hat \theta^{(1)}, …, \hat \theta^{(100)}$ to generate nowcasts for observed clade counts, $\hat C$, by sampling from multinomial distributions. Specifically, the hub will generate predictions for observed clade counts $\hat C^{(1)}, …, \hat C^{(100)}$ where each $\hat C^{(s)}$ is drawn from a $Multinomial(N, \hat \theta^{(s)})$ distribution.


## Simple example

```{r}
n_samp = 100 ## number of samples to use from predictive distribution
alpha = c(2, 4, 4) ## dirichlet params
```

We have a setting where three variants are being predicted. Let us assume that a model's underlying predictive distribution for $\theta$ is $f(\theta) =$ Dirichlet(`r paste(alpha, collapse=",")`), so the expected value of a proportion from this distribution is (`r paste(alpha/sum(alpha), collapse=", ")`) for Variants A, B and C, respectively. Below, we have this model generate $N =$ `r n_samp` samples, $\hat \theta^{(1)}, \dots, \hat \theta^{`r paste(n_samp)`}$ for $\theta$, that are drawn independently from $f(\theta)$.



The following ternary plot shows the true predictive distribution of theta (in colored contours) as well as the `r n_samp` samples drawn.

```{r, eval=FALSE, echo=FALSE}
## remotes::install_github('marvinschmitt/ggsimplex')
library(ggsimplex)
library(ggplot2)

## sample 10 dirichlet
set.seed(123)

library(brms)
data = rdirichlet(n = 10, alpha = alpha)
data = as.data.frame(data)
colnames(data) = c("pmp_1", "pmp_2", "pmp_3")

data$pmp = with(data, make_list_column(pmp_1, pmp_2, pmp_3))

ggplot() +
  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+
  theme_void() +
  geom_simplex_canvas() + 
  geom_simplex_point(data = data, aes(pmp = pmp),
                     size = 0.7, color = "firebrick", alpha = 0.8)

df_dirichlet = data.frame(true_model = 1)
df_dirichlet$Alpha = list(c(2, 4, 8))
ggplot() +
  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+
  theme_void() +
  geom_simplex_canvas() + 
  stat_simplex_density(data=df_dirichlet, fun = ddirichlet,
                       args = alist(Alpha=Alpha))

ggplot() +
  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+
  theme_void() +
  geom_simplex_canvas() + 
  stat_simplex_density(data=df_dirichlet, fun = ddirichlet, col_scale = "linear",
                       args = alist(Alpha=Alpha)) +
  geom_simplex_point(data = data, aes(pmp = pmp),
                   size = 0.7, color = "firebrick", alpha = 0.8)


```


```{r show-predictive-distribution}
library(Ternary)
par(mar = c(0.3, 0.3, 1.3, 0.3))

## make the plot
TernaryPlot(alab = "Variant A prevalence \u2192", 
            blab = "\u2190 Variant B prevalence ", 
            clab = "Variant C prevalence \u2192",
            main = "True predictive distribution with samples",
            region = Ternary:::ternRegionDefault/100,
            point = "right", lab.cex = 0.8, grid.minor.lines = 0,
            grid.lty = "solid", col = rgb(0.9, 0.9, 0.9), grid.col = "white", 
            axis.col = rgb(0.6, 0.6, 0.6), ticks.col = rgb(0.6, 0.6, 0.6),
            axis.rotate = FALSE,
            padding = 0.08)

## add colors
cols <- TernaryPointValues(Func = dd_func, alpha = alpha)
ColourTernary(cols,
              spectrum = rev(hcl.colors(10, palette = "viridis", alpha = 0.6)))

## draw datapoints
thetas <- lapply(FUN = draw_one_dirichlet, 
                 X = rep(1, n_samp), 
                 alpha = alpha)

AddToTernary(graphics::points, 
             thetas,
             pch=20)
```


## Scoring count observations using predictive samples

```{r}
c_1 <- c(1, 2, 2)
c_2 <- c(10, 20, 20)
c_3 <- c(100, 200, 200)
```

Now, let's say that there are three observations of counts $C_1 = (`r paste(c_1, collapse=", ")`)$, $C_2 = (`r paste(c_2, collapse=", ")`)$ and $C_3 = (`r paste(c_3, collapse=", ")`)$ of the three variants. We wish to score the model's predictions (as represented by these `r n_samp` samples), for these observations. Note that these points correspond to the same point in the sample space for a prediction of $\theta$ but not for a prediction of $C$ because they have different total counts.

For each of the predictive samples, there is an implied multinomial distribution conditional on a given $N$. For this simple situation, we can compute the exact multinomial distribution and plot it it directly.


## Evaluation for N=5
Here are a selection of 12 of the individual implied multinomial distributions (i.e. 12 draws from the predictive distribution) with N=5, where the size of the point corresponds to the mass of the pmf. A red x marks where the observation is.

```{r plot-5-sample-multinomial}
par(mfrow=c(3, 4), mar=c(.1, .1, .1, .1))
for(i in 1:12)
  plot_implied_multinomial(n=sum(c_1), theta=thetas[[i]], c=c_1)
```

We can also compute and plot the mixture of implied multinomial distributions. While only 12 samples are shown above, the plot below on the left shows the mixture of all `r n_samp` samples. To generate this distribution, we compute the exact probabilities for each implied multinomial and average them at each point in the sample space. For comparison, on the right we plot the Dirichlet-multinomial distribution based on the parameters from the known true underlying predictive distribution.


```{r plot-mixture-1}
par(mfrow=c(1,2), mar=c(.1, .1, .1, .1))
plot_implied_multinomial_mixture(n=sum(c_1), theta=thetas, c=c_1)
plot_dirichlet_multinomial(n=sum(c_1), alpha=alpha, c=c_1)
```


## Evaluation for N=50

Now we can plot 12 of the implied multinomial distributions for each of the `r n_samp` samples with N=50.
```{r plot-50-sample-multinomial}
par(mfrow=c(3, 4), mar=c(.1, .1, .1, .1))
for(i in 1:12)
  plot_implied_multinomial(n=sum(c_2), theta=thetas[[i]], c=c_2)
```

And again we plot the mixture of implied multinomials and the true Dirichlet-multinomial.
```{r plot-mixture-2}
par(mfrow=c(1,2), mar=c(.1, .1, .1, .1))
plot_implied_multinomial_mixture(n=sum(c_2), theta=thetas, c=c_2)
plot_dirichlet_multinomial(n=sum(c_2), alpha=alpha, c=c_2)
```

## Evaluation for N=500

Here is the mixture of implied multinomials and the true Dirichlet-multinomial for a larger sample size.
```{r plot-mixture-3}
## this takes a while to run, could turn off for faster dev.
par(mfrow=c(1,2), mar=c(.1, .1, .1, .1))
plot_implied_multinomial_mixture(n=sum(c_3), theta=thetas, c=c_3)
plot_dirichlet_multinomial(n=sum(c_3), alpha=alpha, c=c_3)
```



## Conclusions and observations

1. When $N$ is small, the mixture of implied multinomials appears to approximate the true Dirichlet-multinomial better than when $N$ is larger, where there are definite gaps between the distributions from each sample.

## Still to do
- with one (or more) observations: 
  - calculate energy score based on true D-M
  - calculate energy score based on mixture of exact multinomials
  - calculate energy score based on drawing a sample of size K from each multinomial
  - calculate log score based on true D-M
  - calculate log score based on mixture of multinomials
